#!/bin/bash

# vmspawn - Batch VM creation tool for OpenShift CNV
# Creates DataVolume, VolumeSnapshot, and VirtualMachine resources
# with unique batch IDs to avoid naming conflicts across runs.
# Usage: vmspawn <options> [<total-VMs> [<namespaces>]]

set -eu

# Close inherited file descriptors above stderr.
# Prevents subshell deadlocks when invoked from environments (e.g. git hooks,
# CI runners) that leave extra fds open for lock files or IPC.
for _fd in 3 4 5 6 7 8 9; do
    eval "exec $_fd>&-" 2>/dev/null || true
done

# Configuration variables - modify these as needed
DATASOURCE="rhel9"
DATASOURCE_NS="openshift-virtualization-os-images"
DV_URL=
STORAGE_SIZE="32Gi"
STORAGE_CLASS="ocs-storagecluster-ceph-rbd-virtualization"
ACCESS_MODE="ReadWriteMany"
SNAPSHOT_CLASS="ocs-storagecluster-rbdplugin-snapclass"
USE_SNAPSHOT=1
STORAGE_CLASS_SET=0
ACCESS_MODE_SET=0
WAIT_FOR_FIRST_CONSUMER=0
SNAPSHOT_CLASS_SET=0
SNAPSHOT_EXPLICITLY_SET=0
BASE_PVC_NAME="vm-base"
VM_BASENAME="vm"
BASENAME_SET=0
PVC_BASE_NAME_SET=0
VM_CPU_CORES="1"
VM_MEMORY="1Gi"
# Default to using the templates/ directory relative to this script.
CREATE_VM_PATH=${CREATE_VM_PATH:-$(dirname "$0")/templates}
VM_CPU_REQUEST=
VM_MEMORY_REQUEST=
CLOUDINIT_FILE=
# Generate a random 6-character hex batch ID (16.7M unique values)
if [[ -z "$(type -t xxd)" ]] ; then
    echo "Error: xxd command is not installed (needed to generate batch IDs)"
    exit 1
fi
BATCH_ID=$(head -c 3 /dev/urandom | xxd -p)
NUM_VMS=1
NUM_VMS_PER_NAMESPACE=
NUM_NAMESPACES=1
RUN_STRATEGY=Always
WAIT=0
CONTAINERDISK_IMAGE=
RECREATE_EXISTING_VMS=0
DELETE_BATCH=
DELETE_ALL=0
CONFIRM_DELETE=0
PROFILE_TARGET=
yamlpath=()
declare -A expected_vms=()
# If CREATE_VM_PATH is explicitly provided, search for
# YAML files on that path.
# Split the CREATE_VM_PATH into an array using : as the delimiter
IFS=: read -r -a yamlpath <<< "$CREATE_VM_PATH"

doit=1
quiet=0

fatal() {
    echo "$*"
    exit 1
}

# | | | represents spaces that creates indentation for te continuation lines
help() {
    cat <<EOF
Usage: $0 [options] [number_of_vms (default $NUM_VMS) [number_of_namespaces (default $NUM_NAMESPACES)]]
    options:
        -n                      Show what commands would be run
                                without running them
        -q                      Only list objects to be created
        --datasource=NAME       Clone from OCP DataSource (default: $DATASOURCE)
                                Overridden by --dv-url for custom images
        --dv-url=URL            Import disk from URL instead of DataSource
        --storage-size=N        Storage size ($STORAGE_SIZE)
        --storage-class=class   Storage class ($STORAGE_CLASS)
                                | | | (auto-disables snapshots unless
                                | | | --snapshot-class is also provided)
        --access-mode=MODE      PVC access mode (auto-detected from
                                | | | StorageProfile; fallback: $ACCESS_MODE)
                                | | | Shortcuts: --rwx, --rwo
        --snapshot-class=class  Snapshot class ($SNAPSHOT_CLASS)
                                | | | (implies --snapshot)
        --snapshot              Use VolumeSnapshots for cloning (default
                                | | | when using the built-in OCS storage class)
        --no-snapshot           Clone VMs directly from PVC (no snapshot needed)
        --pvc-base-name=name    base PVC name ($BASE_PVC_NAME)
        --batch-id=ID           Batch ID for this run ($BATCH_ID)
                                (auto-generated if not specified)
        --basename=name         VM basename ($VM_BASENAME)
        --cores=N               VM CPU cores ($VM_CPU_CORES)
        --memory=N              VM memory size ($VM_MEMORY)
        --request-memory=N      VM memory request (VM memory size)
        --request-cpu=N         VM CPU request (VM CPU cores)
        --vms=N                 Number of VMs ($NUM_VMS)
        --vms-per-namespace=N   Number of VMs per namespace (not set)
        --namespaces=N          Number of namespaces ($NUM_NAMESPACES)
        --run-strategy=strategy Run strategy ($RUN_STRATEGY)
        --create-existing-vm    Re-apply all VMs even if they already exist
                                | | | Use with --batch-id to update an existing
                                | | | batch (e.g. to resize CPU or memory)
        --wait                  Wait for VMs to start
        --nowait                Do not wait for VMs to start (default)
        --start                 Start the VMs
                                (equivalent to --run-strategy=Always)
        --stop                  Do not start the VMs
                                (equivalent to --run-strategy=Halted)
        --containerdisk[=IMAGE] Use a container disk image instead of PVC-backed
                                | | | storage -- no storage class required.
                                | | | Default image:
                                | | | quay.io/containerdisks/fedora:latest
        --cloudinit=FILE        Inject cloud-init user-data from FILE
                                into each VM (e.g. helpers/cloudinit-stress-workload.yaml)
        --profile[=COMPONENT]   Profile KubeVirt control plane during
                                | | | VM creation (CPU, memory, goroutine, etc.)
                                | | | COMPONENT: virt-api, virt-controller,
                                | | | virt-handler, virt-operator (default: all)
        --delete=BATCH_ID       Delete all resources for the given batch
        --delete-all            Delete ALL vmspawn batches on the cluster
        -y / --yes              Skip confirmation prompt for delete operations
    Specify either --vms (total VMs) or --vms-per-namespace.
    --vms-per-namespace takes precedence.
EOF
    exit 0
}

usage_error() {
    echo "$*" >&2
    echo "Try '$0 --help' for more information." >&2
    exit 1
}

# Process long options.  Handle options names case-insensitive
# and allow either -, _, or neither in names.
process_option() {
    local optstr=${1:-}
    local option=
    local value=
    IFS=$'=' read -r option value <<< "$optstr"
    # Normalize the option name to use - instead of _
    option=${option//_/-}
    option=${option//-/}
    # Convert the option to lowercase for case-insensitive matching
    case "${option,,}" in
	batchid)   BATCH_ID=$value ;;
	delete)    [[ -n "$value" ]] || fatal "--delete requires a batch ID (e.g. --delete=abc123)"
		   DELETE_BATCH=$value ;;
	deleteall) DELETE_ALL=1 ;;
	yes)       CONFIRM_DELETE=1 ;;
	datasourc*) DATASOURCE=$value ;;
	dvurl)     DV_URL=$value; DATASOURCE= ;;
	storagecl*) STORAGE_CLASS=$value; STORAGE_CLASS_SET=1 ;;
	storagesi*|storage) STORAGE_SIZE=$value ;;
	accessmod*|accessmode) ACCESS_MODE=$value; ACCESS_MODE_SET=1 ;;
	rwx) ACCESS_MODE=ReadWriteMany; ACCESS_MODE_SET=1 ;;
	rwo) ACCESS_MODE=ReadWriteOnce; ACCESS_MODE_SET=1 ;;
	snapshotcl*) SNAPSHOT_CLASS=$value; SNAPSHOT_CLASS_SET=1 ;;
	nosnapshot) USE_SNAPSHOT=0; SNAPSHOT_EXPLICITLY_SET=1 ;;
	snapshot) USE_SNAPSHOT=1; SNAPSHOT_EXPLICITLY_SET=1 ;;
	pvc*)      BASE_PVC_NAME=$value; PVC_BASE_NAME_SET=1 ;;
	base*)	   VM_BASENAME=$value; BASENAME_SET=1 ;;
	core*)	   VM_CPU_CORES=$value ;;
	mem*)	   VM_MEMORY=$value ;;
	requestm*) VM_MEMORY_REQUEST=$value ;;
	requestc*) VM_CPU_REQUEST=$value ;;
	vmsper*)   NUM_VMS_PER_NAMESPACE=$value ;;
	vms*)	   NUM_VMS=$value ;;
	namesp*)   NUM_NAMESPACES=$value ;;
	runstrat*) RUN_STRATEGY=$value ;;
	wait)	   WAIT=1 ;;
	nowait)	   WAIT=0 ;;
	cloudinit) CLOUDINIT_FILE=$value ;;
	containerdisk) CONTAINERDISK_IMAGE=${value:-quay.io/containerdisks/fedora:latest}
		   DATASOURCE=; DV_URL= ;;
	createex*) RECREATE_EXISTING_VMS=1 ;;
	start)     RUN_STRATEGY=Always ;;
	stop)      RUN_STRATEGY=Halted ;;
	profile)   PROFILE_TARGET=${value:-all} ;;
	*) 	   fatal "Error: unrecognized option '--${optstr%%=*}'. Run '$0 -h' to see all options." ;;
    esac
}

# process all the options one by one, - is treated as the option"-"
# Leading : suppresses getopts default error so we can show our own message.
while getopts ':nqyh-:' opt "$@" ; do
    case "$opt" in
	n) doit=0		    ;;
	q) quiet=1; doit=0	    ;;
	y) CONFIRM_DELETE=1	    ;;
	h) help			    ;;
	-) process_option "$OPTARG" ;;
	*) fatal "Error: unrecognized option '-${OPTARG:-$opt}'. Run '$0 -h' to see all options." ;;
    esac
done

# clean all the optional arguments that have been processed by getopts so that the positional arguments are now the remaining arguments
shift $((OPTIND-1))

# Detect options misplaced after positional arguments.  Bash getopts
# stops at the first non-option word, so any --option that appears
# after a positional argument is silently treated as a positional
# argument instead of being parsed.  Catch that mistake early.
for _arg in "$@"; do
    if [[ "$_arg" == --* ]]; then
	fatal "Misplaced option '$_arg': all options must appear before positional arguments. Run '$0 -h' for usage."
    fi
done

# Auto-derive VM_BASENAME from the DataSource name when the user
# hasn't explicitly set --basename.
if [[ -n "$DATASOURCE" ]] && (( ! BASENAME_SET )) ; then
    VM_BASENAME="$DATASOURCE"
elif [[ -n "$CONTAINERDISK_IMAGE" ]] && (( ! BASENAME_SET )) ; then
    _cdimg=${CONTAINERDISK_IMAGE##*/}
    VM_BASENAME="${_cdimg%%:*}"
fi

# Always keep BASE_PVC_NAME in sync with VM_BASENAME unless the user
# explicitly set --pvc-base-name.  Previously this derivation only ran
# inside the DATASOURCE block, so --dv-url + --basename left
# BASE_PVC_NAME at the default "vm-base" while the DV template created
# a PVC named "{VM_BASENAME}-base", causing VolumeSnapshot to reference
# a non-existent PVC.
if (( ! PVC_BASE_NAME_SET )) ; then
    BASE_PVC_NAME="${VM_BASENAME}-base"
fi

# --containerdisk is mutually exclusive with --datasource and --dv-url
if [[ -n "$CONTAINERDISK_IMAGE" ]] && { [[ -n "$DATASOURCE" ]] || [[ -n "$DV_URL" ]] ; } ; then
    fatal "--containerdisk cannot be combined with --datasource or --dv-url"
fi

# Auto-disable snapshots when a custom storage class is used without
# a matching snapshot class.  The hardcoded SNAPSHOT_CLASS only works
# with the default OCS STORAGE_CLASS, so using a different storage class
# without providing a snapshot class means snapshots won't work.
if (( ! SNAPSHOT_EXPLICITLY_SET && STORAGE_CLASS_SET && ! SNAPSHOT_CLASS_SET )); then
    USE_SNAPSHOT=0
fi

# Validate arguments early -- before creating log files or contacting the
# cluster -- so pure input errors are reported instantly.
if [[ -z "$DELETE_BATCH" ]] && (( ! DELETE_ALL )) ; then
    if (($# > 2)) ; then
	usage_error "Error: too many positional arguments (expected at most 2, got $#)"
    fi

    if [[ -n "${1:-}" ]] && ! [[ "$1" =~ ^[0-9]+$ ]]; then
	fatal "Invalid argument '$1': expected a number for total VMs"
    fi
    if [[ -n "${2:-}" ]] && ! [[ "$2" =~ ^[0-9]+$ ]]; then
	fatal "Invalid argument '$2': expected a number for namespaces"
    fi

    NUM_VMS=${1:-$NUM_VMS}
    NUM_NAMESPACES=${2:-$NUM_NAMESPACES}

    if [[ -n "$NUM_VMS_PER_NAMESPACE" ]] ; then
	NUM_VMS=$((NUM_VMS_PER_NAMESPACE * NUM_NAMESPACES))
    fi

    if ! [[ "$NUM_VMS" =~ ^[0-9]+$ && "$NUM_VMS" -ge 1 ]]; then
	usage_error "Error: Number of VMs must be a positive integer"
    fi

    if ! [[ "$NUM_NAMESPACES" =~ ^[0-9]+$ && "$NUM_NAMESPACES" -ge 1 ]]; then
	usage_error "Error: Number of namespaces must be a positive integer"
    fi

    if ((NUM_VMS < NUM_NAMESPACES)) ; then
	usage_error "Error: Number of VMs must be greater than or equal to number of namespaces"
    fi
fi

# Setup logging
LOG_DIR="logs"
LOG_FILE="${LOG_DIR}/${BATCH_ID}-${VM_BASENAME}-$(date +%Y-%m-%dT%H:%M:%S).log"

# Function to log with timestamp
# Output may be provided on command line; if not, it is
# read line by line from stdin with timestamp prepended.
# Only timestamp if we're doing something; if it's a dry
# run, the timestamp won't be interesting.
_log_message() {
    # avoid conflicts with the global OPTIND variable
    local -i OPTIND=0
    local -i from_stdin=0
    while getopts v- opt "$@" ; do
	case "$opt" in
	    -) from_stdin=1 ;;
	    *)		    ;;
	esac
    done
    shift $((OPTIND-1))
    if ((from_stdin)) ; then
	local line
	while IFS= read -r line ; do
	    if ((doit)) ; then
		printf "%(%Y-%m-%d %H:%M:%S)T %s\n" -1 "$line"
	    else
		echo "$line"
	    fi
	done
    else
	if ((doit)) ; then
	    printf "%(%Y-%m-%d %H:%M:%S)T %s\n" -1 "$*"
	else
	    echo "$*"
	fi
    fi
}

log_message() {
    if ((doit)) ; then
	_log_message "$*" | tee -a "$LOG_FILE"
    else
	_log_message "$*"
    fi
}

if ((doit)) ; then
    # Create log directory if it doesn't exist
    if [[ ! -d "$LOG_DIR" ]]; then
	mkdir -p "$LOG_DIR"
	echo "Created log directory: $LOG_DIR"
    fi

    log_message "Log file created: $LOG_FILE"
else
    # In dry-run mode, save generated YAML to a file for reference
    if (( ! quiet )) ; then
	if [[ ! -d "$LOG_DIR" ]]; then
	    mkdir -p "$LOG_DIR"
	fi
	DRYRUN_FILE="${LOG_DIR}/${BATCH_ID}-dryrun.yaml"
    fi
fi

# Check for prerequisites: oc CLI, OpenShift Virtualization,
# ODF storage, and the configured storage class.
# Reports all missing prerequisites before exiting.
check_prerequisites() {
    local -i errors=0

    if [[ -z "$(type -t oc)" ]] ; then
	echo "Error: oc command is not installed on system"
	((errors++))
    elif ! oc whoami >/dev/null 2>&1 ; then
	echo "Error: not logged into an OpenShift cluster (run 'oc login' first)"
	((errors++))
    else
	if ! oc get ns openshift-cnv >/dev/null 2>&1 ; then
	    echo "Error: OpenShift Virtualization is not installed (openshift-cnv namespace not found)"
	    ((errors++))
	fi

	if [[ -z "$CONTAINERDISK_IMAGE" ]] ; then
	    if ((USE_SNAPSHOT)) ; then
		if ! oc get ns openshift-storage >/dev/null 2>&1 ; then
		    echo "Error: OpenShift Data Foundation is not installed (openshift-storage namespace not found)"
		    echo "  Hint: use --no-snapshot to skip VolumeSnapshot-based cloning"
		    ((errors++))
		fi
	    fi

	    if ! oc get storageclass "$STORAGE_CLASS" >/dev/null 2>&1 ; then
		echo "Error: storage class '$STORAGE_CLASS' not found"
		echo "  Available storage classes:"
		oc get storageclass --no-headers 2>/dev/null | awk '{print "    " $1}' || true
		((errors++))
	    fi

	    if ((USE_SNAPSHOT)) ; then
		if ! oc get volumesnapshotclass "$SNAPSHOT_CLASS" >/dev/null 2>&1 ; then
		    echo "Error: volume snapshot class '$SNAPSHOT_CLASS' not found"
		    echo "  Available volume snapshot classes:"
		    oc get volumesnapshotclass --no-headers 2>/dev/null | awk '{print "    " $1}' || true
		    echo "  Hint: use --no-snapshot to skip VolumeSnapshot-based cloning"
		    ((errors++))
		fi
	    fi

	    if [[ -n "$DATASOURCE" ]] ; then
		if ! oc get datasource "$DATASOURCE" -n "$DATASOURCE_NS" >/dev/null 2>&1 ; then
		    echo "Error: DataSource '$DATASOURCE' not found in namespace '$DATASOURCE_NS'"
		    echo "  Available DataSources:"
		    oc get datasource -n "$DATASOURCE_NS" --no-headers 2>/dev/null | awk '{print "    " $1}' || true
		    ((errors++))
		fi
	    fi
	fi
    fi

    if ((errors > 0)) ; then
	fatal "Prerequisite check failed with $errors error(s)"
    fi

    if [[ -n "$CONTAINERDISK_IMAGE" ]] ; then
	log_message "Prerequisites OK: oc CLI, OpenShift Virtualization (container disk mode)"
    else
	local snap_msg=""
	if ((USE_SNAPSHOT)) ; then
	    snap_msg=", snapshot class '$SNAPSHOT_CLASS'"
	else
	    snap_msg=" (no-snapshot mode)"
	fi
	log_message "Prerequisites OK: oc CLI, OpenShift Virtualization, storage class '$STORAGE_CLASS'${snap_msg}"
    fi
}

# Auto-detect the PVC access mode from the CDI StorageProfile.
# CDI creates a StorageProfile for each StorageClass containing
# the recommended accessModes and volumeMode.  Since our templates
# use volumeMode: Block, we prefer the access mode from the Block
# property set.  Falls back to the first listed access mode, then
# to the existing ACCESS_MODE default.
# Skipped when the user explicitly sets --access-mode/--rwo/--rwx.
detect_access_mode() {
    if ((ACCESS_MODE_SET)) ; then
	log_message "Access mode explicitly set to: $ACCESS_MODE"
	return
    fi

    local detected=""

    # Try to get the access mode for Block volumeMode first
    detected=$(oc get storageprofile "$STORAGE_CLASS" \
	-o go-template='{{range .status.claimPropertySets}}{{if eq .volumeMode "Block"}}{{index .accessModes 0}}{{break}}{{end}}{{end}}' \
	2>/dev/null) || true

    # Fall back to the first access mode in the StorageProfile
    if [[ -z "$detected" ]] ; then
	detected=$(oc get storageprofile "$STORAGE_CLASS" \
	    -o jsonpath='{.status.claimPropertySets[0].accessModes[0]}' \
	    2>/dev/null) || true
    fi

    if [[ -n "$detected" ]] ; then
	ACCESS_MODE="$detected"
	log_message "Auto-detected access mode '$ACCESS_MODE' from StorageProfile for '$STORAGE_CLASS'"
    else
	log_message "Warning: Could not detect access mode from StorageProfile for '$STORAGE_CLASS'; using default: $ACCESS_MODE"
    fi

    # Detect WaitForFirstConsumer storage classes.  With WFFC the base
    # DataVolume PVC won't bind until a consumer Pod is scheduled.
    # Snapshot mode is incompatible with WFFC because the base PVC has
    # no consumer Pod -- it would deadlock.  Auto-disable snapshots and
    # fall back to direct DataSource clone which has no base PVC.
    local bind_mode
    bind_mode=$(oc get storageclass "$STORAGE_CLASS" -o jsonpath='{.volumeBindingMode}' 2>/dev/null) || true
    if [[ "$bind_mode" == "WaitForFirstConsumer" ]] ; then
	WAIT_FOR_FIRST_CONSUMER=1
	log_message "Storage class '$STORAGE_CLASS' uses WaitForFirstConsumer binding."
	if ((USE_SNAPSHOT)) ; then
	    USE_SNAPSHOT=0
	    log_message "  Warning: Disabling snapshot mode -- snapshots require a bound base PVC,"
	    log_message "  but WFFC storage won't bind until a consumer Pod is scheduled."
	    log_message "  Falling back to direct DataSource clone (no base PVC needed)."
	else
	    log_message "  Will skip waiting for base DataVolume and proceed to VM creation"
	    log_message "  so that consumer Pods can trigger PVC binding."
	fi
    fi
}

# Check if template files exist
find_file_on_path() {
    local file=$1
    local ydir
    for ydir in "${yamlpath[@]}" ; do
	if [[ -f "${ydir:-.}/$file" ]] ; then
	    echo "${ydir:-.}/$file"
	    return 0
	fi
    done
    return 1
}

check_file_exists() {
    find_file_on_path "${1:-}" >/dev/null || fatal "${1:-} not found on $CREATE_VM_PATH"
}

# Validate batch ID format for --delete (Kubernetes label value rules:
# alphanumeric, dots, hyphens, underscores; 1-63 chars; starts/ends alphanumeric).
# This blocks wildcards, commas, spaces, and other special characters that
# could confuse label selectors or cause unexpected behavior.
if [[ -n "$DELETE_BATCH" ]] ; then
    if ! [[ "$DELETE_BATCH" =~ ^[a-zA-Z0-9]([a-zA-Z0-9._-]{0,61}[a-zA-Z0-9])?$ ]]; then
	fatal "Invalid batch ID '$DELETE_BATCH': must be 1-63 alphanumeric characters (plus . _ -), starting and ending with alphanumeric"
    fi
fi

# --delete and --delete-all are mutually exclusive
if [[ -n "$DELETE_BATCH" ]] && ((DELETE_ALL)) ; then
    fatal "Cannot use --delete and --delete-all together"
fi

# --profile is incompatible with --delete and --delete-all
if [[ -n "$PROFILE_TARGET" ]] ; then
    if [[ -n "$DELETE_BATCH" ]] || ((DELETE_ALL)) ; then
	fatal "Cannot use --profile with --delete or --delete-all"
    fi
    case "$PROFILE_TARGET" in
	all|virt-api|virt-controller|virt-handler|virt-operator) ;;
	*) fatal "Invalid --profile value '$PROFILE_TARGET'. Valid values: virt-api, virt-controller, virt-handler, virt-operator, all" ;;
    esac
fi

# Skip creation-specific checks when deleting
if [[ -z "$DELETE_BATCH" ]] && (( ! DELETE_ALL )) ; then
    if ((doit)) ; then
	check_prerequisites
    fi

    if [[ -z "$CONTAINERDISK_IMAGE" ]] ; then
	detect_access_mode
    fi

    check_file_exists namespace.yaml
    if [[ -n "$CONTAINERDISK_IMAGE" ]] ; then
	check_file_exists vm-containerdisk.yaml
    elif ((USE_SNAPSHOT)) ; then
	check_file_exists volumesnap.yaml
	check_file_exists vm-snap.yaml
    elif [[ -n "$DATASOURCE" ]] ; then
	# No-snapshot + DataSource: each VM clones directly from the
	# DataSource, so no intermediate base DV or vm-clone template needed.
	check_file_exists vm-datasource.yaml
    else
	check_file_exists vm-clone.yaml
    fi
    if [[ -z "$CONTAINERDISK_IMAGE" ]] ; then
	if [[ -n "$DATASOURCE" ]] ; then
	    # Base DV is only needed in snapshot mode (to create snapshots from).
	    # In no-snapshot mode each VM's DV clones directly from the DataSource.
	    if ((USE_SNAPSHOT)) ; then
		check_file_exists dv-datasource.yaml
	    fi
	else
	    if [[ -z "$DV_URL" ]] ; then
		fatal "Either --datasource, --dv-url, or --containerdisk must be set"
	    fi
	    check_file_exists dv.yaml
	fi
    fi

    # Validate cloud-init file if specified
    if [[ -n "$CLOUDINIT_FILE" && ! -f "$CLOUDINIT_FILE" ]] ; then
	fatal "Cloud-init file not found: $CLOUDINIT_FILE"
    fi

    # Calculate VMs per namespace
    VMS_PER_NAMESPACE=$((NUM_VMS / NUM_NAMESPACES))
    REMAINDER_VMS=$((NUM_VMS % NUM_NAMESPACES))

    cat <<EOF
Creating resources for:
  Batch ID: $BATCH_ID
  Total VMs: $NUM_VMS
  Namespaces: $NUM_NAMESPACES
  VMs per namespace: $VMS_PER_NAMESPACE
  Extra VMs in first $REMAINDER_VMS namespaces: $((REMAINDER_VMS > 0 ? 1 : 0))
  Disk source: ${DATASOURCE:+DataSource $DATASOURCE ($DATASOURCE_NS)}${DV_URL:+URL $DV_URL}${CONTAINERDISK_IMAGE:+ContainerDisk $CONTAINERDISK_IMAGE}${DV_URL:+
  Storage Size: $STORAGE_SIZE}
  Storage Class: $(if [[ -n "$CONTAINERDISK_IMAGE" ]]; then echo "N/A (container disk)"; else echo "$STORAGE_CLASS"; fi)
  Access Mode: $(if [[ -n "$CONTAINERDISK_IMAGE" ]]; then echo "N/A (container disk)"; else echo "$ACCESS_MODE"; fi)
  Snapshot mode: $(if [[ -n "$CONTAINERDISK_IMAGE" ]]; then echo "N/A (container disk)"; elif ((USE_SNAPSHOT)); then echo "enabled (class: $SNAPSHOT_CLASS)"; elif [[ -n "$DATASOURCE" ]]; then echo "disabled (direct DataSource clone)"; else echo "disabled (direct PVC clone)"; fi)
  VM Basename: $VM_BASENAME
  VM CPU Cores: $VM_CPU_CORES
  VM Memory: $VM_MEMORY
  Cloud-init: ${CLOUDINIT_FILE:-none}

EOF
fi

do_oc() {
    if ((doit)) ; then
	oc apply -f -
    elif ((quiet)) ; then
	local line
	while read -r line ; do : ; done
    else
	if [[ -n "${DRYRUN_FILE:-}" ]] ; then
	    tee -a "$DRYRUN_FILE"
	    echo "---" >> "$DRYRUN_FILE"
	else
	    cat
	fi
    fi
}

# Replace templated variables in YAML files with their values.
function process_template() {
    local file=$1
    sed -e "s/{vm-ns}/$namespace/g" \
        -e "s/{vm-id}/${VM_ID:-}/g" \
        -e "s/{VM_BASENAME}/$VM_BASENAME/g" \
        -e "s/{BATCH_ID}/$BATCH_ID/g" \
        -e "s|{DV_URL}|$DV_URL|g" \
        -e "s/{DATASOURCE}/${DATASOURCE:-}/g" \
        -e "s/{DATASOURCE_NS}/${DATASOURCE_NS:-}/g" \
        -e "s/{BASE_PVC_NAME}/$BASE_PVC_NAME/g" \
        -e "s/{STORAGE_SIZE}/$STORAGE_SIZE/g" \
        -e "s/{STORAGE_CLASS}/$STORAGE_CLASS/g" \
        -e "s/{ACCESS_MODE}/$ACCESS_MODE/g" \
        -e "s/{SNAPSHOT_CLASS}/$SNAPSHOT_CLASS/g" \
        -e "s/{VM_CPU_CORES}/$VM_CPU_CORES/g" \
        -e "s/{VM_MEMORY}/$VM_MEMORY/g" \
        -e "s/{RUN_STRATEGY}/$RUN_STRATEGY/g" \
        -e "s|{CONTAINERDISK_IMAGE}|$CONTAINERDISK_IMAGE|g" \
        "$file"
}

# Indent replacement text based on indentation of
# a templated token.  Purpose is to allow indentation of
# YAML fragments based on indentation of the template
# in the text stream.
indent_token() {
    local token=$1
    local text=$2
    local line
    while IFS='' read -r line ; do
	if [[ $line =~ ^([ ]+)\{$token\}$ ]] ; then
	    local prefix=${BASH_REMATCH[1]}
	    while IFS='' read -r repl ; do
		if [[ -n "$repl" ]] ; then
		    echo "${prefix}${repl}"
		fi
	    done <<< "$text"
	else
	    echo "$line"
	fi
    done
}

################################################################
# Function to create namespaces
create_namespaces() {
    log_message "Creating namespaces..."
    local -i ns
    local ns_file
    ns_file=$(find_file_on_path "namespace.yaml") || fatal "Can't find namespace.yaml on CREATE_VM_PATH"
    local -A existing_namespaces=()
    if ((doit)) ; then
	local namespace
	while read -r namespace ; do
	    if [[ -n "$namespace" ]] ; then existing_namespaces["$namespace"]=1; fi
	done <<< "$(oc get namespace --no-headers 2>/dev/null | awk '{print $1}')"
    fi
    for ((ns=1; ns<=NUM_NAMESPACES; ns++)); do
        namespace="vm-${BATCH_ID}-ns-${ns}"

        # Check if namespace already exists
	if [[ -n "${existing_namespaces[$namespace]:-}" ]] ; then
            log_message "Namespace $namespace already exists, skipping creation"
        else
            log_message "Creating namespace: $namespace"
            process_template "$ns_file" | do_oc
        fi
    done
}

################################################################
# DataVolumes
check_datavolume_status() {
    local namespace=$1
    local datavolume_name="${VM_BASENAME}-base"

    log_message "Checking DataVolume status in namespace: $namespace"

    # Wait for DataVolume to be completed
    while :; do
	local status
        status=$(oc get datavolume "$datavolume_name" -n "$namespace" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Pending")

        case $status in
            "Succeeded")
                log_message "DataVolume $datavolume_name in namespace $namespace is completed"
                return 0
                ;;
            "Failed")
                fatal "Error: DataVolume $datavolume_name in namespace $namespace failed"
                ;;
            "Pending"|"PendingPopulation"|"Running"|"ImportScheduled"|"ImportInProgress"|"CloneScheduled"|"CloneInProgress"|"SnapshotForSmartCloneInProgress"|"SmartClonePVCInProgress")
                log_message "DataVolume $datavolume_name in namespace $namespace is still $status, waiting..."
                sleep 10
                ;;
            *)
                log_message "DataVolume $datavolume_name in namespace $namespace has unknown status: $status"
                sleep 10
                ;;
        esac
    done
}

# Function to wait for all DataVolumes to complete
wait_for_all_datavolumes() {
    log_message "Waiting for all DataVolumes to complete..."

    local -i ns
    for ((ns=1; ns<=NUM_NAMESPACES; ns++)); do
        namespace="vm-${BATCH_ID}-ns-${ns}"

        if ! check_datavolume_status "$namespace"; then
            fatal "Error: Failed to create DataVolume in namespace $namespace"
        fi
    done

    log_message "All DataVolumes are completed successfully!"
}

create_datavolumes() {
    # In no-snapshot mode with a DataSource, each VM's DataVolumeTemplate
    # clones directly from the DataSource -- no intermediate base DV needed.
    if [[ -n "$DATASOURCE" ]] && ! ((USE_SNAPSHOT)) ; then
	log_message "Skipping base DataVolume creation (each VM clones directly from DataSource)"
	return
    fi

    log_message "Creating DataVolumes..."
    local -i ns
    local dv_file
    if [[ -n "$DATASOURCE" ]] ; then
	dv_file=$(find_file_on_path "dv-datasource.yaml") || fatal "Can't find dv-datasource.yaml on CREATE_VM_PATH"
    else
	dv_file=$(find_file_on_path "dv.yaml") || fatal "Can't find dv.yaml on CREATE_VM_PATH"
    fi
    for ((ns=1; ns<=NUM_NAMESPACES; ns++)); do
        namespace="vm-${BATCH_ID}-ns-${ns}"

        log_message "Creating DataVolume for namespace: $namespace"
        process_template "$dv_file" | do_oc
    done
    if ((doit)) ; then
	if ((WAIT_FOR_FIRST_CONSUMER)) ; then
	    log_message "Skipping DataVolume wait (WaitForFirstConsumer); VMs will trigger PVC binding."
	else
	    wait_for_all_datavolumes
	fi
    fi
}

################################################################
# VolumeSnapshots
check_volumesnapshot_status() {
    local namespace=$1
    local snapshot_name="${VM_BASENAME}-${namespace}"

    log_message "Checking VolumeSnapshot status in namespace: $namespace"

    # Wait for VolumeSnapshot to be ready
    while :; do
	local status
        status=$(oc get volumesnapshot "$snapshot_name" -n "$namespace" -o jsonpath='{.status.readyToUse}' 2>/dev/null || echo "false")

	case "$status" in
	    "true")
		log_message "VolumeSnapshot $snapshot_name in namespace $namespace is ready"
		return 0
		;;
	    *)
		log_message "VolumeSnapshot $snapshot_name in namespace $namespace is not ready yet, waiting..."
		sleep 10
		;;
	esac
    done
}

# Function to wait for all VolumeSnapshots to be ready
wait_for_all_volumesnapshots() {
    log_message "Waiting for all VolumeSnapshots to be ready..."

    local -i ns
    for ((ns=1; ns<=NUM_NAMESPACES; ns++)); do
        namespace="vm-${BATCH_ID}-ns-${ns}"

        if ! check_volumesnapshot_status "$namespace"; then
            fatal "Error: Failed to create VolumeSnapshot in namespace $namespace"
        fi
    done

    log_message "All VolumeSnapshots are ready successfully!"
}

create_volumesnapshots() {
    log_message "Creating VolumeSnapshots..."
    local -i ns
    local vs_file
    vs_file=$(find_file_on_path "volumesnap.yaml") || fatal "Can't find volumesnap.yaml on CREATE_VM_PATH"
    for ((ns=1; ns<=NUM_NAMESPACES; ns++)); do
        namespace="vm-${BATCH_ID}-ns-${ns}"
        log_message "Creating VolumeSnapshot for namespace: $namespace"
        process_template "$vs_file" | do_oc
    done
    if ((doit)) ; then wait_for_all_datavolumes; fi
}

################################################################
# VMs
wait_for_all_vms() {
    log_message "Waiting for all VMs to be ready"
    local -i total_vms="${#expected_vms[@]}"
    local -i ready_vms=0

    while : ; do
	# We expect to have a lot more VMs than we do other objects, which are
	# have only one per namespace.  Therefore the algorithm of waiting in turn for
	# each one to be ready will be inefficient, and we use the method of listing
	# all VMs that are ready and checking them off our list.
	while read -r vm ; do
	    if [[ -n "$vm" && -n "${expected_vms[$vm]:-}" ]] ; then
		unset "expected_vms[$vm]"
		ready_vms=$((ready_vms + 1))
	    fi
	done <<< "$(oc get vm -A --no-headers | awk '{if ($5 == "True" && $4 == "Running") {printf "%s/%s\n", $1, $2}}')"
	if (("${#expected_vms[@]}" == 0)) ; then
	    log_message "All VMs are ready"
	    return
	else
	    log_message "${ready_vms}/${total_vms} ready"
	    sleep 60
	fi
    done
}

create_virtualmachines() {
    log_message "Creating VirtualMachines..."
    VM_ID=1
    local vm_file
    if [[ -n "$CONTAINERDISK_IMAGE" ]] ; then
	vm_file=$(find_file_on_path "vm-containerdisk.yaml") || fatal "Can't find vm-containerdisk.yaml on CREATE_VM_PATH"
    elif ((USE_SNAPSHOT)) ; then
	vm_file=$(find_file_on_path "vm-snap.yaml") || fatal "Can't find vm-snap.yaml on CREATE_VM_PATH"
    elif [[ -n "$DATASOURCE" ]] ; then
	vm_file=$(find_file_on_path "vm-datasource.yaml") || fatal "Can't find vm-datasource.yaml on CREATE_VM_PATH"
    else
	vm_file=$(find_file_on_path "vm-clone.yaml") || fatal "Can't find vm-clone.yaml on CREATE_VM_PATH"
    fi

    # Auto-apply default cloud-init when using DataSource or container disk
    # without an explicit --cloudinit, so VMs are accessible via SSH out of the box.
    if [[ ( -n "$DATASOURCE" || -n "$CONTAINERDISK_IMAGE" ) && -z "$CLOUDINIT_FILE" ]] ; then
	CLOUDINIT_FILE="$(dirname "$0")/helpers/cloudinit-default.yaml"
	if [[ ! -f "$CLOUDINIT_FILE" ]] ; then
	    fatal "Default cloud-init file not found: $CLOUDINIT_FILE"
	fi
	log_message "No --cloudinit specified; applying default cloud-init (root password: password)"
    fi
    local requeststr=
    local -A requests=()
    if [[ -n "${VM_CPU_REQUEST:-}" ]] ; then requests[cpu]=$VM_CPU_REQUEST; fi
    if [[ -n "${VM_MEMORY_REQUEST:-}" ]] ; then requests[memory]=$VM_MEMORY_REQUEST; fi
    if [[ -n "${requests[*]}" ]] ; then
	requeststr="
resources:
  requests:
$(local resource; for resource in "${!requests[@]}" ; do echo "    $resource: ${requests[$resource]}" ; done)
"
    fi
    # Build cloud-init YAML fragments if a cloud-init file is specified.
    # Userdata is stored in a Secret per namespace (to avoid the 2048-byte
    # inline limit) and referenced via userDataSecretRef.
    local cloudinit_disk=""
    local cloudinit_volume=""
    local cloudinit_b64=""
    local cloudinit_secret_file=""
    if [[ -n "$CLOUDINIT_FILE" ]] ; then
	cloudinit_b64=$(base64 -w0 "$CLOUDINIT_FILE")
	cloudinit_secret_file=$(find_file_on_path "cloudinit-secret.yaml") ||
	    fatal "Can't find cloudinit-secret.yaml on CREATE_VM_PATH"
	cloudinit_disk="- disk:
    bus: virtio
  name: cloudinit"
	cloudinit_volume="- cloudInitNoCloud:
    secretRef:
      name: ${VM_BASENAME}-cloudinit
  name: cloudinit"
    fi
    local -A existing_vms=()
    if ((! RECREATE_EXISTING_VMS)) ; then
	local vm
	while read -r vm ; do
	    if [[ -n "$vm" ]] ; then existing_vms["$vm"]=1; fi
	done <<< "$(oc get vm -A --no-headers | awk '{printf "%s/%s\n", $1, $2}')"
    fi
    for ((ns=1; ns<=NUM_NAMESPACES; ns++)); do
        namespace="vm-${BATCH_ID}-ns-${ns}"

	# Create cloud-init Secret in this namespace if needed
	if [[ -n "$cloudinit_secret_file" ]] ; then
	    log_message "Creating cloud-init Secret in namespace: $namespace"
	    process_template "$cloudinit_secret_file" |
		sed "s|{CLOUDINIT_B64}|$cloudinit_b64|g" |
		do_oc ||
		fatal "Cannot create cloud-init secret in $namespace!"
	fi

        # Calculate VMs for this namespace
        vms_in_this_namespace=$VMS_PER_NAMESPACE
        if [[ $ns -le $REMAINDER_VMS ]]; then
            vms_in_this_namespace=$((vms_in_this_namespace + 1))
        fi

        # Create VMs for this namespace
        for ((vm=1; vm<=vms_in_this_namespace; vm++)); do
	    local vm_name="${namespace}/${VM_BASENAME}-${BATCH_ID}-${VM_ID}"
	    if [[ -z "${existing_vms[$vm_name]:-}" ]] ; then
		log_message "Creating VirtualMachine $VM_ID for namespace: $namespace"
		process_template "$vm_file" |
		    indent_token RESOURCES "$requeststr" |
		    indent_token CLOUDINIT_DISK "$cloudinit_disk" |
		    indent_token CLOUDINIT_VOLUME "$cloudinit_volume" |
		    do_oc ||
		    fatal "Cannot create vm $vm_name!"
	    fi
	    expected_vms["$vm_name"]=1
            VM_ID=$((VM_ID + 1))
        done
    done
    if ((doit)) ; then
	if ((WAIT)) ; then
	    wait_for_all_vms
	else
	    log_message "Not waiting for all VMs"
	fi
    fi
}

################################################################
# Verify cloud-init completed successfully inside each VM.
# Uses virtctl ssh + sshpass for non-interactive access.
# Polls in rounds (30s apart, up to 5 min) until each VM
# reports "done" or "error".  Gracefully skips if tools
# are missing.
verify_cloudinit() {
    if [[ -z "$(type -t virtctl)" ]]; then
	log_message "Note: install 'virtctl' to enable automatic cloud-init verification"
	log_message "  Manual check: virtctl ssh -n <ns> root@<vm> -c 'cloud-init status --long'"
	return 0
    fi
    if [[ -z "$(type -t sshpass)" ]]; then
	log_message "Note: install 'sshpass' to enable automatic cloud-init verification"
	log_message "  Manual check: virtctl ssh -n <ns> root@<vm> -c 'cloud-init status --long'"
	return 0
    fi

    # Collect VMs for this batch from the cluster
    local -a all_vms=()
    local ns vm_name
    while read -r ns vm_name _rest; do
	[[ -n "$vm_name" ]] || continue
	all_vms+=("$ns/$vm_name")
    done <<< "$(oc get vm -A -l batch-id="$BATCH_ID" --no-headers 2>/dev/null | awk '{print $1, $2}')"

    local -i total=${#all_vms[@]}
    if ((total == 0)); then return 0; fi

    # Sample up to 3 random VMs to keep verification fast at any scale
    local -i max_sample=3
    local -A vm_pending=()
    if ((total <= max_sample)); then
	for vm_key in "${all_vms[@]}"; do
	    vm_pending["$vm_key"]=1
	done
    else
	local -A picked=()
	while ((${#picked[@]} < max_sample)); do
	    local idx=$((RANDOM % total))
	    if [[ -z "${picked[$idx]:-}" ]]; then
		picked[$idx]=1
		vm_pending["${all_vms[$idx]}"]=1
	    fi
	done
    fi

    local -i sample=${#vm_pending[@]}
    log_message "Verifying cloud-init on $sample of $total VM(s) (polling up to 5 minutes)..."

    local -i ok_count=0
    local -i fail_count=0
    local -i round
    local vm_key ci_status

    for ((round=1; round<=10; round++)); do
	((${#vm_pending[@]} > 0)) || break
	if ((round > 1)); then sleep 30; fi

	for vm_key in "${!vm_pending[@]}"; do
	    ns="${vm_key%%/*}"
	    vm_name="${vm_key##*/}"

	    ci_status=$(sshpass -p password virtctl ssh -n "$ns" \
		--local-ssh-opts="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10" \
		root@"$vm_name" --command "cloud-init status" 2>/dev/null) || ci_status=""

	    if [[ "$ci_status" == *"done"* ]]; then
		log_message "  $vm_name: cloud-init OK"
		unset "vm_pending[$vm_key]"
		((ok_count++))
	    elif [[ "$ci_status" == *"error"* ]]; then
		log_message "  WARNING: $vm_name: cloud-init FAILED"
		unset "vm_pending[$vm_key]"
		((fail_count++))
	    fi
	done

	if ((${#vm_pending[@]} > 0)); then
	    log_message "  $((ok_count + fail_count))/${sample} checked, ${#vm_pending[@]} still running..."
	fi
    done

    # Remaining VMs timed out
    for vm_key in "${!vm_pending[@]}"; do
	vm_name="${vm_key##*/}"
	log_message "  WARNING: $vm_name: cloud-init verification timed out"
	((fail_count++))
    done

    if ((fail_count > 0)); then
	log_message "Cloud-init: $ok_count OK, $fail_count FAILED or timed out (sampled $sample of $total)"
	log_message "  Inspect: virtctl ssh -n <ns> root@<vm> -c 'cloud-init status --long'"
    else
	log_message "Cloud-init: $sample of $total VM(s) sampled -- all OK"
    fi
}

################################################################
# Delete all resources for a batch
# Args: $1 = batch ID, $2 = "skip-confirm" (optional, used by delete_all_batches)
delete_batch() {
    local batch=$1
    local skip_confirm=${2:-}

    if ((! doit)) ; then
        echo "(dry-run) Would delete all resources for batch '$batch':"
        echo "  oc delete ns -l batch-id=$batch"
        echo ""
        echo "This deletes all namespaces, VMs, DataVolumes, VolumeSnapshots,"
        echo "and PVCs belonging to batch '$batch'."
        return 0
    fi

    # Show what will be deleted
    echo "Resources for batch '$batch':"
    echo ""
    echo "Namespaces:"
    oc get ns -l batch-id="$batch" --no-headers 2>/dev/null || true
    echo ""
    echo "VirtualMachines:"
    oc get vm -A -l batch-id="$batch" --no-headers 2>/dev/null || true
    echo ""

    # Check if anything exists
    local ns_count
    ns_count=$(oc get ns -l batch-id="$batch" --no-headers 2>/dev/null | wc -l)
    if ((ns_count == 0)) ; then
        echo "No resources found for batch '$batch'"
        return 0
    fi

    # Safety: verify all matching namespaces look like vmspawn-created ones.
    # This prevents accidental deletion of operator/system namespaces that
    # may have been manually tagged with a batch-id label.
    local ns_name
    while read -r ns_name ; do
	if [[ -n "$ns_name" && ! "$ns_name" =~ ^vm-[a-zA-Z0-9._-]+-ns-[0-9]+$ ]] ; then
	    fatal "Refusing to delete namespace '$ns_name': does not match vmspawn naming pattern (vm-{batch}-ns-{N})"
	fi
    done <<< "$(oc get ns -l batch-id="$batch" --no-headers 2>/dev/null | awk '{print $1}')"

    # Confirmation prompt (unless --yes or called from delete_all_batches)
    if (( ! CONFIRM_DELETE )) && [[ "$skip_confirm" != "skip-confirm" ]] ; then
	read -r -p "Delete these resources? [y/N] " confirm
	if [[ "${confirm,,}" != "y" ]] ; then
	    echo "Aborted."
	    return 0
	fi
    fi

    # Delete namespaces (cascades VMs, DVs, snapshots, PVCs)
    log_message "Deleting namespaces for batch '$batch'..."
    oc delete ns -l batch-id="$batch"
    log_message "All resources for batch '$batch' deleted"

    # Clean up manifest file
    local manifest="${LOG_DIR}/batch-${batch}.manifest"
    if [[ -f "$manifest" ]] ; then
        rm -f "$manifest"
        log_message "Removed manifest: $manifest"
    fi
}

################################################################
# Delete ALL vmspawn batches on the cluster
delete_all_batches() {
    # Discover all vmspawn-created namespaces by the batch-id label,
    # then extract unique batch IDs from the namespace names.
    local all_batches

    if ((! doit)) ; then
	cat <<-EOF
	(dry-run) Would discover and delete all vmspawn batches.
	  oc get ns -l batch-id --no-headers

	This finds all namespaces with a batch-id label matching
	the vmspawn naming pattern (vm-{batch}-ns-{N}) and deletes them.
	EOF
	return 0
    fi

    all_batches=$(oc get ns -l batch-id --no-headers 2>/dev/null |
	awk '{print $1}' |
	grep -oP '^vm-\K[a-zA-Z0-9._-]+(?=-ns-[0-9]+$)' |
	sort -u) || true

    if [[ -z "$all_batches" ]] ; then
	echo "No vmspawn batches found on the cluster."
	return 0
    fi

    echo "Found vmspawn batches:"
    local batch
    while read -r batch ; do
	local ns_count
	ns_count=$(oc get ns -l batch-id="$batch" --no-headers 2>/dev/null | wc -l)
	local vm_count
	vm_count=$(oc get vm -A -l batch-id="$batch" --no-headers 2>/dev/null | wc -l)
	echo "  $batch  ($ns_count namespaces, $vm_count VMs)"
    done <<< "$all_batches"
    echo ""

    # Confirmation (unless --yes)
    if (( ! CONFIRM_DELETE )) ; then
	read -r -p "Delete ALL batches above? This is irreversible. [y/N] " confirm
	if [[ "${confirm,,}" != "y" ]] ; then
	    echo "Aborted."
	    return 0
	fi
    fi

    # Delete each batch; skip per-batch confirmation since we already confirmed
    while read -r batch ; do
	delete_batch "$batch" skip-confirm
    done <<< "$all_batches"

    log_message "All vmspawn batches deleted."
}

################################################################
# Cluster profiler functions
PROFILER_BIN=
PROFILER_DOWNLOAD_URL="https://github.com/gqlo/vmspawn/releases/latest/download/cluster-profiler-linux-amd64"

ensure_profiler_binary() {
    # 1. Check if cluster-profiler is on PATH
    if command -v cluster-profiler &>/dev/null ; then
	PROFILER_BIN=$(command -v cluster-profiler)
	log_message "Using cluster-profiler from PATH: $PROFILER_BIN"
	return 0
    fi

    # 2. Check cached binary
    local cache_dir="$HOME/.vmspawn/bin"
    local cached_bin="$cache_dir/cluster-profiler"
    if [[ -x "$cached_bin" ]] ; then
	PROFILER_BIN="$cached_bin"
	log_message "Using cached cluster-profiler: $PROFILER_BIN"
	return 0
    fi

    # 3. Download pre-built binary from GitHub Releases
    log_message "Downloading cluster-profiler binary..."
    mkdir -p "$cache_dir"

    if command -v curl &>/dev/null ; then
	curl -fsSL -o "$cached_bin" "$PROFILER_DOWNLOAD_URL" ||
	    fatal "Failed to download cluster-profiler from $PROFILER_DOWNLOAD_URL"
    elif command -v wget &>/dev/null ; then
	wget -q -O "$cached_bin" "$PROFILER_DOWNLOAD_URL" ||
	    fatal "Failed to download cluster-profiler from $PROFILER_DOWNLOAD_URL"
    else
	fatal "Neither curl nor wget is available. Install cluster-profiler manually or install curl/wget."
    fi

    chmod +x "$cached_bin"
    PROFILER_BIN="$cached_bin"
    log_message "cluster-profiler binary downloaded to: $PROFILER_BIN"
}

ensure_profiler_feature_gate() {
    # Detect OCP/CNV vs upstream KubeVirt
    local hco_exists
    hco_exists=$(oc get hco kubevirt-hyperconverged -n openshift-cnv -o name 2>/dev/null) || true

    if [[ -n "$hco_exists" ]] ; then
	# OCP/CNV: check via jsonpatch annotation on HCO CR
	local annotation
	annotation=$(oc get hco kubevirt-hyperconverged -n openshift-cnv \
	    -o jsonpath='{.metadata.annotations.kubevirt\.kubevirt\.io/jsonpatch}' \
	    2>/dev/null) || true

	if echo "$annotation" | grep -q "ClusterProfiler" ; then
	    log_message "ClusterProfiler feature gate is already enabled (OCP/CNV)"
	    return 0
	fi

	cat <<-EOF

	ClusterProfiler feature gate is not enabled on this cluster.
	This is required for profiling KubeVirt control-plane components.

	EOF
	read -r -p "Enable ClusterProfiler feature gate now? [y/N] " confirm
	if [[ "${confirm,,}" != "y" ]] ; then
	    fatal "Profiling requires the ClusterProfiler feature gate. Aborting."
	fi

	log_message "Enabling ClusterProfiler feature gate via HCO annotation..."
	if [[ -z "$annotation" || "$annotation" == "null" ]] ; then
	    # No existing jsonpatch annotation -- simple case
	    oc annotate --overwrite -n openshift-cnv hco kubevirt-hyperconverged \
		'kubevirt.kubevirt.io/jsonpatch=[{"op": "add", "path": "/spec/configuration/developerConfiguration/featureGates/-", "value": "ClusterProfiler"}]' ||
		fatal "Failed to enable ClusterProfiler feature gate"
	else
	    # Existing annotation present -- need to merge
	    # Parse the existing JSON array and append the ClusterProfiler entry
	    local new_entry='{"op": "add", "path": "/spec/configuration/developerConfiguration/featureGates/-", "value": "ClusterProfiler"}'
	    local merged
	    # Remove trailing ] and append new entry
	    merged="${annotation%]}, ${new_entry}]"
	    oc annotate --overwrite -n openshift-cnv hco kubevirt-hyperconverged \
		"kubevirt.kubevirt.io/jsonpatch=${merged}" ||
		fatal "Failed to enable ClusterProfiler feature gate"
	fi
	log_message "ClusterProfiler feature gate enabled. Waiting for rollout..."
	sleep 5
    else
	# Upstream KubeVirt: check feature gates directly on KubeVirt CR
	local feature_gates
	feature_gates=$(oc get kubevirt kubevirt -n kubevirt \
	    -o jsonpath='{.spec.configuration.developerConfiguration.featureGates}' \
	    2>/dev/null) || true

	if echo "$feature_gates" | grep -q "ClusterProfiler" ; then
	    log_message "ClusterProfiler feature gate is already enabled (upstream KubeVirt)"
	    return 0
	fi

	cat <<-EOF

	ClusterProfiler feature gate is not enabled on this cluster.
	This is required for profiling KubeVirt control-plane components.

	EOF
	read -r -p "Enable ClusterProfiler feature gate now? [y/N] " confirm
	if [[ "${confirm,,}" != "y" ]] ; then
	    fatal "Profiling requires the ClusterProfiler feature gate. Aborting."
	fi

	log_message "Enabling ClusterProfiler feature gate on KubeVirt CR..."
	oc patch kubevirt kubevirt -n kubevirt --type=json \
	    -p '[{"op": "add", "path": "/spec/configuration/developerConfiguration/featureGates/-", "value": "ClusterProfiler"}]' ||
	    fatal "Failed to enable ClusterProfiler feature gate"
	log_message "ClusterProfiler feature gate enabled. Waiting for rollout..."
	sleep 5
    fi
}

profiler_start() {
    log_message "Starting KubeVirt control-plane profiling..."
    "$PROFILER_BIN" --cmd start ||
	fatal "Failed to start cluster profiling"
    log_message "Profiling started successfully"
}

profiler_prompt_and_stop() {
    echo ""
    echo "================================================================"
    echo "  Cluster profiling is in progress"
    echo "  KubeVirt control plane profiling is active (CPU, memory, etc.)"
    echo "================================================================"
    echo ""
    read -r -p "Press Enter to stop profiling and dump results (or Ctrl+C to abort)..."
    echo ""

    log_message "Stopping KubeVirt control-plane profiling..."
    "$PROFILER_BIN" --cmd stop ||
	fatal "Failed to stop cluster profiling"
    log_message "Profiling stopped"

    local output_dir="${LOG_DIR}/profile-${BATCH_ID}"
    log_message "Dumping profiling results to $output_dir..."

    if [[ "$PROFILE_TARGET" == "all" ]] ; then
	"$PROFILER_BIN" --cmd dump --output-dir "$output_dir" ||
	    fatal "Failed to dump profiling results"
    else
	# region agent log
	echo "{\"id\":\"log_dump_cmd\",\"timestamp\":$(date +%s%3N),\"location\":\"vmspawn:1266\",\"message\":\"profiler dump command\",\"data\":{\"target\":\"$PROFILE_TARGET\",\"cmd\":\"$PROFILER_BIN --cmd dump --l kubevirt.io=$PROFILE_TARGET --output-dir $output_dir\"},\"hypothesisId\":\"A\"}" >> /home/guoqingli/work/vmspawn/.cursor/debug.log
	# endregion
	"$PROFILER_BIN" --cmd dump --l "kubevirt.io=$PROFILE_TARGET" --output-dir "$output_dir" ||
	    fatal "Failed to dump profiling results for $PROFILE_TARGET"
    fi

    log_message "Profiling results saved to: $output_dir"
    echo ""
    echo "Profiling results: $output_dir"
    echo "Analyze with: go tool pprof $output_dir/<pod>/<profile>.pprof"
    echo "Plot with:    go tool pprof -svg $output_dir/<pod>/cpu.pprof > cpu.svg"
}

################################################################
# Main execution
main() {
    log_message "Starting resource creation process..."
    log_message "Batch ID:      $BATCH_ID"
    log_message "Configuration: $NUM_VMS VMs across $NUM_NAMESPACES namespaces"

    if [[ -n "$CONTAINERDISK_IMAGE" ]] ; then
	log_message "ContainerDisk: $CONTAINERDISK_IMAGE"
    elif [[ -n "$DATASOURCE" ]] ; then
	log_message "DataSource:    $DATASOURCE (from $DATASOURCE_NS)"
    else
	log_message "DV URL:        $DV_URL"
	log_message "Storage size:  $STORAGE_SIZE"
    fi
    if [[ -z "$CONTAINERDISK_IMAGE" ]] ; then
	log_message "Storage class: $STORAGE_CLASS"
	if ((USE_SNAPSHOT)) ; then
	    log_message "Snapshot mode: enabled (class: $SNAPSHOT_CLASS)"
	elif [[ -n "$DATASOURCE" ]] ; then
	    log_message "Snapshot mode: disabled (direct DataSource clone)"
	else
	    log_message "Snapshot mode: disabled (direct PVC clone)"
	fi
    fi
    log_message "VM CPU cores:  $VM_CPU_CORES"
    log_message "VM memory:     $VM_MEMORY"
    log_message "Cloud-init:    ${CLOUDINIT_FILE:-none}"
    log_message "Run strategy:  $RUN_STRATEGY"

    create_namespaces
    if [[ -z "$CONTAINERDISK_IMAGE" ]] ; then
	create_datavolumes
	if ((USE_SNAPSHOT)) ; then
	    create_volumesnapshots
	else
	    log_message "Skipping VolumeSnapshots (--no-snapshot mode)"
	fi
    fi
    create_virtualmachines

    # TODO: re-enable once verify_cloudinit is improved
    # if ((doit && WAIT)) && [[ -n "$CLOUDINIT_FILE" ]]; then
    #     verify_cloudinit
    # fi

    if ((doit)) ; then
	log_message "Resource creation completed successfully!"
	if [[ -n "$CONTAINERDISK_IMAGE" ]] ; then
	    log_message "Created $NUM_NAMESPACES namespaces and $NUM_VMS total VirtualMachines (container disk)"
	elif ((USE_SNAPSHOT)) ; then
	    log_message "Created $NUM_NAMESPACES namespaces, $NUM_NAMESPACES DataVolumes, $NUM_NAMESPACES VolumeSnapshots, and $NUM_VMS total VirtualMachines"
	elif [[ -n "$DATASOURCE" ]] ; then
	    log_message "Created $NUM_NAMESPACES namespaces and $NUM_VMS total VirtualMachines (direct DataSource clone, no base DVs)"
	else
	    log_message "Created $NUM_NAMESPACES namespaces, $NUM_NAMESPACES DataVolumes, and $NUM_VMS total VirtualMachines (no snapshots)"
	fi

	# Write batch manifest file
	local manifest_file="${LOG_DIR}/batch-${BATCH_ID}.manifest"
	local ns_list=""
	local vm_list=""
	for ((ns=1; ns<=NUM_NAMESPACES; ns++)); do
	    local ns_name="vm-${BATCH_ID}-ns-${ns}"
	    ns_list="${ns_list:+${ns_list}, }${ns_name}"
	done
	for key in "${!expected_vms[@]}" ; do
	    vm_list="${vm_list:+${vm_list}, }${key}"
	done
	cat > "$manifest_file" <<-MANIFEST
	batch-id: $BATCH_ID
	created: $(date +%Y-%m-%dT%H:%M:%S)
	basename: $VM_BASENAME
	total-vms: $NUM_VMS
	total-namespaces: $NUM_NAMESPACES
	namespaces: $ns_list
	vms: $vm_list
	MANIFEST
	log_message "Batch manifest written to: $manifest_file"

	cat <<-EOF

	Batch ID: $BATCH_ID

	To check the created resources:
	  oc get ns -l batch-id=$BATCH_ID
	EOF
	if [[ -z "$CONTAINERDISK_IMAGE" ]] ; then
	    echo "  oc get datavolumes -A -l batch-id=$BATCH_ID"
	fi
	if ((USE_SNAPSHOT)) ; then
	    echo "  oc get volumesnapshots -A -l batch-id=$BATCH_ID"
	fi
	cat <<-EOF
	  oc get vm -A -l batch-id=$BATCH_ID

	To delete all resources in this batch:
	  oc delete vm -A -l batch-id=$BATCH_ID
	  oc delete ns -l batch-id=$BATCH_ID

	To list all batches:
	  ls ${LOG_DIR}/*.manifest
	EOF
    fi
}

# Run delete or create
if (( DELETE_ALL )) ; then
    delete_all_batches
elif [[ -n "$DELETE_BATCH" ]] ; then
    delete_batch "$DELETE_BATCH"
else
    if [[ -n "$PROFILE_TARGET" ]] ; then
	if ((doit)) ; then
	    ensure_profiler_binary
	    ensure_profiler_feature_gate
	    profiler_start
	else
	    echo "(dry-run) Would ensure cluster-profiler binary is available"
	    echo "(dry-run) Would check/enable ClusterProfiler feature gate"
	    echo "(dry-run) Would start profiling before VM creation"
	fi
    fi
    main
    if [[ -n "$PROFILE_TARGET" ]] ; then
	if ((doit)) ; then
	    profiler_prompt_and_stop
	else
	    echo "(dry-run) Would prompt to stop profiling and dump results"
	fi
    fi
fi

# Print dry-run YAML file location
if [[ -n "${DRYRUN_FILE:-}" && -f "${DRYRUN_FILE:-}" ]] ; then
    echo ""
    echo "Dry-run YAML saved to: $DRYRUN_FILE"
fi
